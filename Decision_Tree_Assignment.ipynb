{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree  Assignment\n"
      ],
      "metadata": {
        "id": "egbZ-8vmMAr0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        "\n",
        "-> In machine learning, a Decision Tree is a non-parametric supervised learning algorithm used for both classification and regression tasks. It resembles an upside-down tree or a flowchart, where each internal branch represents a choice based on data features, leading to a final predicted outcome\n",
        "\n",
        "\n",
        "Q2:  Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "\n",
        "-> Gini Impurity\n",
        "\n",
        "Gini Impurity measures the probability that a randomly chosen element from the dataset would be incorrectly labeled if it were randomly classified according to the class distribution in that node.\n",
        "Formula: \\(G=1-\\sum p_{i}^{2}\\), where \\(p_{i}\\) is the probability of an element belonging to class \\(i\\).Range: Typically ranges from 0 to 0.5 for binary classification.0: Perfect purity (all elements belong to one class).0.5: Maximum impurity (elements are evenly distributed among classes).Split Impact: It favors larger partitions and is computationally efficient because it does not involve logarithmic calculations. It is the default metric for the CART (Classification and Regression Trees) algorithm.\n",
        "\n",
        " Entropy\n",
        "\n",
        " Entropy originates from information theory and measures the degree of uncertainty or randomness in a dataset.\n",
        "Formula: \\(H=-\\sum p_{i}\\log _{2}(p_{i})\\).Range: Ranges from 0 to 1.0: No uncertainty (pure node).1: Maximum uncertainty (perfectly mixed classes).Split Impact: It is used to calculate Information Gain, which is the reduction in entropy after a split. Entropy is more sensitive to small changes in class probabilities than Gini and often produces more balanced splits, though it is slower to compute due to the \\(\\log \\) function. It is primarily used in the ID3 and C4.5 algorithms.\n",
        "\n",
        "\n",
        "Q3:  What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each?\n",
        "\n",
        "\n",
        "-> Pre-Pruning (Early Stopping)\n",
        "\n",
        "Pre-pruning halts the growth of a decision tree during the construction phase. It stops further splitting of a node if it does not meet a predefined threshold or criteria.\n",
        "Common Criteria: Setting a maximum depth (max_depth), a minimum number of samples required to split a node (min_samples_split), or a minimum impurity decrease.\n",
        "Practical Advantage: Computational Efficiency. Since the algorithm stops building the tree early, it saves significant time and memory, making it ideal for extremely large datasets where building a full tree would be resource-intensive.\n",
        "\n",
        "Post-Pruning (Backward Pruning)\n",
        "\n",
        "Post-pruning allows the decision tree to grow to its full depth (often perfectly classifying the training set) and then trims away non-significant branches.\n",
        "Common Method: Cost Complexity Pruning, which uses a parameter (often called alpha) to balance tree size against accuracy on a validation set.\n",
        "Practical Advantage: Higher Pruning Quality. It avoids the \"horizon effect,\" where a split that seems non-significant on its own might lead to very valuable splits further down. Post-pruning ensures the model captures these deep, complex patterns before simplifying the structure.\n",
        "\n",
        "\n",
        "Q4: What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "\n",
        "-> In decision tree learning, Information Gain (IG) is a metric used to determine which feature (attribute) most effectively splits data into homogeneous groups. It quantifies the reduction in \"disorder\" or uncertainty after a dataset is partitioned based on a specific feature.\n",
        "\n",
        "The Formula:\\(\\text{Information\\ Gain}=\\text{Entropy(parent)}-\\text{Weighted\\ Average\\ Entropy(children)}\\)\n",
        "\n",
        "This calculation represents the difference between the uncertainty of the dataset before the split and the total uncertainty remaining in the resulting branches.\n",
        " Why It Is Important for Choosing Splits Information Gain serves as the \"selection criterion\" for building the tree. Its primary roles include:\n",
        "\n",
        " Identifying the Best Split: The algorithm calculates the IG for every possible feature and threshold. The feature with the highest Information Gain is chosen as the splitting point for that node because it reduces uncertainty the most.Maximizing Purity: By selecting features that maximize gain, the algorithm ensures that each split creates \"purer\" child nodes where the data points belong more consistently to a single class.Driving Tree Structure: It dictates the order of decision nodes, typically placing the most \"informative\" features (those that clarify the target label fastest) near the root of the tree.\n",
        "\n",
        "\n",
        "\n",
        "Q5:  What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "\n",
        "-> Common Real-World Applications\n",
        "\n",
        "Finance:\n",
        "Banks use them for credit scoring to determine loan eligibility based on income and credit history, as well as for fraud detection by identifying anomalous transaction patterns.\n",
        "Healthcare:\n",
        " Medical professionals apply decision trees for disease diagnosis (e.g., predicting diabetes or heart disease) and creating personalized treatment plans based on patient symptoms and medical history.\n",
        "Marketing & Retail\n",
        ": Companies use them for customer segmentation to target specific demographics and churn prediction to identify customers likely to stop using a service. Retailers also use them for inventory management to predict sales trends.\n",
        "Manufacturing: Used for quality control to identify production conditions that lead to defects and for equipment failure prediction.\n",
        "\n",
        "Main Advantages\n",
        "Interpretability:\n",
        "They provide a \"white box\" model that is easy for non-experts to understand and visualize as a flowchart.\n",
        "\n",
        "Minimal Data Preparation:\n",
        " They do not require feature scaling or normalization and can handle both numerical and categorical data directly.\n",
        "Handling Missing Values:\n",
        " Many decision tree algorithms can effectively handle datasets with missing values or outliers.\n",
        "Versatility: They are applicable to both classification (discrete outcomes) and regression (continuous values) tasks.\n",
        "\n",
        "\n",
        "Q6: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances?\n"
      ],
      "metadata": {
        "id": "o1CgJ3LEMD-_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7ib8-0sKdTf",
        "outputId": "91583660-9450-45d5-8ca6-4216c93887b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0000\n",
            "\n",
            "Feature Importances:\n",
            "             Feature  Importance\n",
            "2  petal length (cm)    0.893264\n",
            "3   petal width (cm)    0.087626\n",
            "1   sepal width (cm)    0.019110\n",
            "0  sepal length (cm)    0.000000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features: sepal/petal length and width\n",
        "y = iris.target  # Labels: Setosa, Versicolor, Virginica\n",
        "\n",
        "# Split data: 70% for training and 30% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Train a Decision Tree Classifier using the Gini criterion\n",
        "# Note: 'gini' is the default criterion for DecisionTreeClassifier in scikit-learn\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 3. Predict and print model accuracy\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# 4. Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': iris.feature_names,\n",
        "    'Importance': clf.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(feature_importance_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree"
      ],
      "metadata": {
        "id": "n5pD-1HROjzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into 70% training and 30% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Train a Decision Tree with max_depth=3 (Pre-pruning)\n",
        "clf_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "acc_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# 3. Train a fully-grown Decision Tree (No depth limit)\n",
        "clf_full = DecisionTreeClassifier(max_depth=None, random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "acc_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# 4. Compare Accuracy and Depth\n",
        "print(f\"--- Depth-Limited Tree (max_depth=3) ---\")\n",
        "print(f\"Test Accuracy: {acc_limited:.4f}\")\n",
        "print(f\"Actual Tree Depth: {clf_limited.get_depth()}\")\n",
        "\n",
        "print(f\"\\n--- Fully-Grown Tree (Unlimited) ---\")\n",
        "print(f\"Test Accuracy: {acc_full:.4f}\")\n",
        "print(f\"Actual Tree Depth: {clf_full.get_depth()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjTaKMIqOZu6",
        "outputId": "191dcd5d-53c4-4ad4-ca77-72ee9de9280f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Depth-Limited Tree (max_depth=3) ---\n",
            "Test Accuracy: 1.0000\n",
            "Actual Tree Depth: 3\n",
            "\n",
            "--- Fully-Grown Tree (Unlimited) ---\n",
            "Test Accuracy: 1.0000\n",
            "Actual Tree Depth: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8: Write a Python program to:\n",
        "● Load the California Housing dataset from sklearn\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importance"
      ],
      "metadata": {
        "id": "KGXv_PnIO5Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split the dataset (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Train a Decision Tree Regressor\n",
        "# We use a max_depth to prevent extreme overfitting, a common issue with trees\n",
        "regressor = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# 3. Predict and Print Mean Squared Error (MSE)\n",
        "y_pred = regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "\n",
        "# 4. Print Feature Importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': housing.feature_names,\n",
        "    'Importance': regressor.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(feature_importance_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "_73gVhVrPEDi",
        "outputId": "b3812c47-eca3-4546-d62c-7d7e3bb8e202"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "HTTP Error 403: Forbidden",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-937140046.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 1. Load the California Housing dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mhousing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_california_housing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhousing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhousing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/datasets/_california_housing.py\u001b[0m in \u001b[0;36mfetch_california_housing\u001b[0;34m(data_home, download_if_missing, return_X_y, as_frame, n_retries, delay)\u001b[0m\n\u001b[1;32m    175\u001b[0m         )\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         archive_path = _fetch_remote(\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0mARCHIVE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mdirname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_home\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/datasets/_base.py\u001b[0m in \u001b[0;36m_fetch_remote\u001b[0;34m(remote, dirname, n_retries, delay)\u001b[0m\n\u001b[1;32m   1511\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1513\u001b[0;31m                 \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1514\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mURLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_splittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    631\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracY?"
      ],
      "metadata": {
        "id": "TncSpjPQPhiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Define the Parameter Grid\n",
        "# max_depth: controls the maximum levels of the tree\n",
        "# min_samples_split: minimum number of samples needed to split a node\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6],\n",
        "    'min_samples_split': [2, 5, 10, 20]\n",
        "}\n",
        "\n",
        "# 3. Initialize GridSearchCV\n",
        "# cv=5 means 5-fold cross-validation is used during the search\n",
        "dtree = DecisionTreeClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(estimator=dtree, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# 4. Tune the Model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 5. Extract results\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# 6. Predict and print resulting accuracy\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Final Model Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCJBYkrZPdYz",
        "outputId": "0c8cb2da-686b-47ce-805c-864434ba343c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Final Model Test Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting."
      ],
      "metadata": {
        "id": "hBleRkZ-PwYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS 10: 1. Handle Missing Values\n",
        "\n",
        "Medical datasets often contain missing values due to skipped tests or recording errors.\n",
        "Identify the Mechanism: Determine if data is Missing Completely at Random (MCAR) or Missing at Random (MAR).\n",
        "Imputation Strategies:\n",
        "\n",
        "Statistical Imputation:\n",
        " Replace missing numeric values with the median (robust to outliers) and categorical values with the mode (most frequent class).\n",
        "\n",
        "Advanced Imputation:\n",
        "Use a secondary model like KNN Imputation or an SVM regressor to predict missing values based on other patient features.\n",
        "\n",
        "Algorithm Support:\n",
        " Modern Decision Tree implementations (like XGBoost or specific CART variants) can handle missing values natively by creating \"surrogate splits\" or assigning them to the most common branch.\n",
        "\n",
        "2. Encode Categorical Features\n",
        "Machine learning models require numerical input.\n",
        "One-Hot Encoding: Use for nominal features without order (e.g., \"Blood Type\" or \"Gender\").\n",
        "\n",
        "Ordinal Encoding: Use for features with a natural ranking (e.g., \"Pain Severity: Low, Medium, High\").\n",
        "Target Encoding: Useful for high-cardinality features (e.g., \"Zip Code\") to prevent sparse data, though it requires care to avoid leakage.\n",
        "\n",
        "3. Train the Decision Tree Model\n",
        "Data Split: Divide your data into training (e.g., 80%) and testing (20%) sets.\n",
        "Criterion Selection: Use Gini Impurity for computational efficiency or Entropy if you need to measure specific information gain.\n",
        "\n",
        "Initialization:\n",
        " Train the DecisionTreeClassifier on the processed training set.\n",
        "\n",
        "4. Tune Hyperparameters\n",
        "Tuning prevents the tree from \"memorizing\" specific patients (overfitting) and ensures it learns general medical patterns.\n",
        "\n",
        "Grid Search (GridSearchCV): Exhaustively test combinations of parameters like max_depth (tree height), min_samples_split (minimum patients needed to split a node), and min_samples_leaf.\n",
        "\n",
        "Pruning: Use Cost Complexity Pruning to trim branches that do not significantly improve the model’s predictive power on validation data.\n",
        "\n",
        "5. Evaluate Performance\n",
        "Accuracy alone is insufficient in healthcare; the cost of a false negative (missing a disease) is much higher than a false positive.\n",
        "\n",
        "Primary Metrics: Focus on Recall (Sensitivity) to ensure most diseased patients are caught and Precision to avoid unnecessary treatments.\n",
        "\n",
        "F1-Score: The harmonic mean of precision and recall, providing a balanced view of model quality.\n",
        "\n",
        "ROC-AUC: Measures the model's ability to distinguish between diseased and healthy patients regardless of the decision threshold.\n",
        "\n",
        "Real-World Business Value\n",
        "Clinical Decision Support:\n",
        " Acts as a \"second opinion\" for doctors, flagging high-risk patients for early intervention.\n",
        "\n",
        "Operational Efficiency:\n",
        "Automates the initial screening process, allowing healthcare providers to prioritize critical cases and reduce wait times.\n",
        "\n",
        "Cost Reduction: Identifies patients who do not need expensive, invasive diagnostic tests, saving resources for both hospitals and insurance providers.\n",
        "Personalized Care: Enables data-driven treatment plans tailored to a patient's specific symptoms and history."
      ],
      "metadata": {
        "id": "Kb3yuCQFP8Ie"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rZzfNwRCPrS2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}